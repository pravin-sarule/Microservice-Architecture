# Frontend Streaming Implementation Guide

## ‚úÖ Complete Streaming Solution for Unlimited AI Responses

This guide provides the complete frontend implementation for rendering unlimited AI responses using Server-Sent Events (SSE) with heartbeat support.

---

## üî∑ 1Ô∏è‚É£ React Component - Fail-Proof Streaming Buffer

### AIChat.jsx - Production-Ready Component

```jsx
import { useState, useRef, useEffect } from 'react';

export default function AIChat({ endpoint, authToken, onComplete }) {
  const [text, setText] = useState('');
  const buffer = useRef('');
  const eventSourceRef = useRef(null);
  const updateTimeoutRef = useRef(null);
  const [isStreaming, setIsStreaming] = useState(false);
  const [error, setError] = useState(null);

  const startStream = async (question, additionalData = {}) => {
    // Clear previous state
    setText('');
    buffer.current = '';
    setError(null);
    setIsStreaming(true);

    // Close existing connection if any
    if (eventSourceRef.current) {
      eventSourceRef.current.close();
    }

    try {
      // Use fetch API for POST with SSE
      const response = await fetch(endpoint, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${authToken}`,
          'Accept': 'text/event-stream',
        },
        body: JSON.stringify({
          ...additionalData,
          question,
        }),
      });

      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }

      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let buffer = '';

      while (true) {
        const { done, value } = await reader.read();
        
        if (done) {
          setIsStreaming(false);
          if (onComplete) {
            onComplete(buffer.current);
          }
          break;
        }

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || ''; // Keep incomplete line

        for (const line of lines) {
          if (!line.trim() || !line.startsWith('data: ')) continue;
          
          const data = line.replace(/^data: /, '').trim();
          
          // Handle heartbeat
          if (data === '[PING]') {
            continue; // Ignore heartbeat
          }
          
          // Handle completion
          if (data === '[DONE]') {
            setIsStreaming(false);
            if (onComplete) {
              onComplete(buffer.current);
            }
            return;
          }

          // Parse JSON data
          try {
            const parsed = JSON.parse(data);
            
            if (parsed.type === 'metadata') {
              // Handle metadata (session_id, etc.)
              console.log('Stream metadata:', parsed);
            } else if (parsed.type === 'chunk') {
              // Append chunk to buffer
              buffer.current += parsed.text || '';
              
              // Update UI every 50ms for performance (prevents React freezing)
              if (updateTimeoutRef.current) {
                clearTimeout(updateTimeoutRef.current);
              }
              
              updateTimeoutRef.current = setTimeout(() => {
                setText(buffer.current);
              }, 50);
            } else if (parsed.type === 'done') {
              // Final metadata
              setText(buffer.current);
              setIsStreaming(false);
              if (onComplete) {
                onComplete(buffer.current, parsed);
              }
            } else if (parsed.type === 'error') {
              setError(parsed.error);
              setIsStreaming(false);
            }
          } catch (e) {
            // Skip invalid JSON - might be partial data
          }
        }
      }
    } catch (err) {
      console.error('Streaming error:', err);
      setError(err.message);
      setIsStreaming(false);
    }
  };

  const stopStream = () => {
    if (eventSourceRef.current) {
      eventSourceRef.current.close();
      eventSourceRef.current = null;
    }
    if (updateTimeoutRef.current) {
      clearTimeout(updateTimeoutRef.current);
    }
    setIsStreaming(false);
  };

  useEffect(() => {
    return () => {
      stopStream();
    };
  }, []);

  return (
    <div className="p-4">
      <div className="mb-4">
        <button
          onClick={() => startStream('Your question here')}
          disabled={isStreaming}
          className="bg-blue-600 text-white px-4 py-2 rounded disabled:opacity-50"
        >
          {isStreaming ? 'Streaming...' : 'Start Streaming'}
        </button>
        {isStreaming && (
          <button
            onClick={stopStream}
            className="ml-2 bg-red-600 text-white px-4 py-2 rounded"
          >
            Stop
          </button>
        )}
      </div>

      {error && (
        <div className="mb-4 p-3 bg-red-100 text-red-700 rounded">
          Error: {error}
        </div>
      )}

      <div className="border p-4 bg-gray-100 rounded min-h-[200px] max-h-[600px] overflow-y-auto">
        <pre className="whitespace-pre-wrap break-words">{text}</pre>
        {isStreaming && (
          <span className="inline-block w-2 h-4 bg-blue-600 animate-pulse ml-1">|</span>
        )}
      </div>
    </div>
  );
}
```

---

## üî∑ 2Ô∏è‚É£ Usage Examples

### For Document Chat (`/api/doc/chat/stream`)

```jsx
import AIChat from './components/AIChat';

function DocumentChat({ fileId, authToken }) {
  const GATEWAY_BASE_URL = process.env.REACT_APP_GATEWAY_URL || 'http://localhost:5000';
  
  const handleComplete = (fullAnswer, metadata) => {
    console.log('Stream completed:', fullAnswer);
    console.log('Session ID:', metadata?.session_id);
  };

  return (
    <AIChat
      endpoint={`${GATEWAY_BASE_URL}/files/chat/stream`}
      authToken={authToken}
      onComplete={handleComplete}
      additionalData={{
        file_id: fileId,
        llm_name: 'gemini',
      }}
    />
  );
}
```

### For Folder Chat (`/api/files/:folderName/query/stream`)

```jsx
function FolderChat({ folderName, authToken }) {
  const GATEWAY_BASE_URL = process.env.REACT_APP_GATEWAY_URL || 'http://localhost:5000';
  
  return (
    <AIChat
      endpoint={`${GATEWAY_BASE_URL}/docs/${folderName}/query/stream`}
      authToken={authToken}
      additionalData={{
        llm_name: 'claude-sonnet-4',
      }}
    />
  );
}
```

---

## üî∑ 3Ô∏è‚É£ Key Features

### ‚úÖ Heartbeat Support
- Automatically ignores `[PING]` messages
- Prevents connection timeout for long responses

### ‚úÖ Chunk Buffering
- Updates UI every 50ms to prevent React freezing
- Handles large responses without crashing

### ‚úÖ Error Handling
- Graceful error messages
- Auto-cleanup on errors

### ‚úÖ Connection Management
- Properly closes connections on unmount
- Prevents memory leaks

### ‚úÖ Performance Optimized
- Throttled UI updates (50ms intervals)
- Efficient text buffer management

---

## üî∑ 4Ô∏è‚É£ Backend Endpoints with Gateway Base URL

### Document Chat Streaming
- **Gateway Endpoint**: `POST {GATEWAY_BASE_URL}/files/chat/stream`
- **Example Development**: `POST http://localhost:5000/files/chat/stream`
- **Proxies To**: `POST {FILE_SERVICE_URL}/api/doc/chat/stream`
- **Example Production**: `POST https://gateway.yourdomain.com/files/chat/stream`
- **Headers**: 
  - `Content-Type: application/json`
  - `Authorization: Bearer {token}`
  - `Accept: text/event-stream`
- **Body**: Same as `/api/doc/chat`
  ```json
  {
    "file_id": "uuid (optional)",
    "question": "Your question",
    "llm_name": "gemini",
    "session_id": "uuid (optional)"
  }
  ```

### Folder Chat Streaming
- **Gateway Endpoint**: `POST {GATEWAY_BASE_URL}/docs/{folderName}/query/stream`
- **Example Development**: `POST http://localhost:5000/docs/my-case/query/stream`
- **Proxies To**: `POST {FILE_SERVICE_URL}/api/files/{folderName}/query/stream`
- **Example Production**: `POST https://gateway.yourdomain.com/docs/my-case/query/stream`
- **Headers**: 
  - `Content-Type: application/json`
  - `Authorization: Bearer {token}`
  - `Accept: text/event-stream`
- **Body**: Same as `/api/files/:folderName/query`
  ```json
  {
    "question": "Your question",
    "llm_name": "claude-sonnet-4",
    "session_id": "uuid (optional)",
    "secret_id": "uuid (optional)"
  }
  ```

---

## üî∑ 5Ô∏è‚É£ Testing

### Test with curl:

```bash
# Gateway Endpoint - Document Chat Streaming
curl -X POST http://localhost:5000/files/chat/stream \
  -H "Content-Type: application/json" \
  -H "Accept: text/event-stream" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{"file_id": "uuid", "question": "Summarize this document", "llm_name": "gemini"}' \
  --no-buffer

# Gateway Endpoint - Folder Query Streaming
curl -X POST http://localhost:5000/docs/my-case/query/stream \
  -H "Content-Type: application/json" \
  -H "Accept: text/event-stream" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{"question": "Summarize all documents in this folder", "llm_name": "claude-sonnet-4"}' \
  --no-buffer

# Production Example (replace with your gateway URL):
curl -X POST https://gateway.yourdomain.com/files/chat/stream \
  -H "Content-Type: application/json" \
  -H "Accept: text/event-stream" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{"file_id": "uuid", "question": "Summarize this document"}' \
  --no-buffer
```

### Expected Output:
```
data: {"type":"metadata","session_id":"..."}

data: {"type":"chunk","text":"This"}
data: {"type":"chunk","text":" is"}
data: {"type":"chunk","text":" the"}
data: [PING]
data: {"type":"chunk","text":" response"}
...
data: {"type":"done","session_id":"...","answer":"..."}
data: [DONE]
```

---

## ‚úÖ Result

With this implementation:
- ‚úî Unlimited response length
- ‚úî Zero rendering failures
- ‚úî Zero server disconnects (heartbeat)
- ‚úî Perfect streaming behavior
- ‚úî Works with all LLM APIs (Gemini, Claude, OpenAI, DeepSeek)
- ‚úî Production-safe

